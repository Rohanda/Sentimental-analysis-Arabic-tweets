{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-11T23:28:08.168406Z","iopub.execute_input":"2021-12-11T23:28:08.168809Z","iopub.status.idle":"2021-12-11T23:29:28.529129Z","shell.execute_reply.started":"2021-12-11T23:28:08.168734Z","shell.execute_reply":"2021-12-11T23:29:28.528402Z"},"_kg_hide-output":true,"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nfrom tqdm import tqdm  # for progress bar \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport csv\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:31:17.328647Z","iopub.execute_input":"2021-12-11T23:31:17.328922Z","iopub.status.idle":"2021-12-11T23:31:17.334785Z","shell.execute_reply.started":"2021-12-11T23:31:17.328895Z","shell.execute_reply":"2021-12-11T23:31:17.334141Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Load dataset","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 280)\ntrain_neg = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_negative_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntrain_neg.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntrain_neg['label'] = 0\n\ntrain_pos = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/train_Arabic_tweets_positive_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntrain_pos.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntrain_pos['label'] = 1\n\n\ntrain_df = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n\n\nfrom sklearn.model_selection import train_test_split\nX = train_df.tweet.values\ny = train_df.label.values\n\n# splitting the data \nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.1, random_state=2020)\n\n# Load test data\ntest_pos = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_positive_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntest_pos.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntest_pos['label']=1\n\ntest_neg = pd.read_csv(\"../input/arabic-sentiment-twitter-corpus/test_Arabic_tweets_negative_20190413.tsv\", sep=\"\\t\", header=None,  quoting=csv.QUOTE_NONE)\ntest_neg.rename(columns={0:'label', 1:'tweet'}, inplace=True)\ntest_neg['label']=0\n\ntest_df = pd.concat([test_neg, test_pos], axis=0).reset_index(drop=True)\nX_test = test_df.tweet.values\ny_test = test_df.label.values","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:31:19.441493Z","iopub.execute_input":"2021-12-11T23:31:19.441747Z","iopub.status.idle":"2021-12-11T23:31:20.355996Z","shell.execute_reply.started":"2021-12-11T23:31:19.441721Z","shell.execute_reply":"2021-12-11T23:31:20.355246Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### arabic-bert-base model was pretrained on ~8.2 Billion words:\n\n* Arabic version of OSCAR - filtered from Common Crawl\n* Recent dump of Arabic Wikipedia\n\nand other Arabic resources which sum up to ~95GB of text.","metadata":{}},{"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:31:29.976829Z","iopub.execute_input":"2021-12-11T23:31:29.977157Z","iopub.status.idle":"2021-12-11T23:31:30.660667Z","shell.execute_reply.started":"2021-12-11T23:31:29.977119Z","shell.execute_reply":"2021-12-11T23:31:30.659783Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:34:18.716893Z","iopub.execute_input":"2021-12-11T23:34:18.717208Z","iopub.status.idle":"2021-12-11T23:34:19.772320Z","shell.execute_reply.started":"2021-12-11T23:34:18.717175Z","shell.execute_reply":"2021-12-11T23:34:19.771561Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Define preprocessing util function\ndef text_preprocessing(text):\n    \n    # Normalize unicode encoding\n    text = unicodedata.normalize('NFC', text)\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    #Remove URLs\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n\n\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:34:19.775980Z","iopub.execute_input":"2021-12-11T23:34:19.776323Z","iopub.status.idle":"2021-12-11T23:34:19.782809Z","shell.execute_reply.started":"2021-12-11T23:34:19.776274Z","shell.execute_reply":"2021-12-11T23:34:19.782000Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Create a function to tokenize a set of texts\nimport emoji\nimport unicodedata\ndef preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing ):\n   \n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n    tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n\n    # For every sentence...\n    for i,sent in enumerate(data):\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing_fn(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate/pad\n            padding='max_length',        # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True,     # Return attention mask\n            truncation = True \n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:34:24.321596Z","iopub.execute_input":"2021-12-11T23:34:24.322486Z","iopub.status.idle":"2021-12-11T23:34:24.353370Z","shell.execute_reply.started":"2021-12-11T23:34:24.322435Z","shell.execute_reply":"2021-12-11T23:34:24.352658Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\nMAX_LEN =  280\n\n# Print sentence 13 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[13]])[0].squeeze().numpy())\nprint('Original: ', X[13])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:35:44.195867Z","iopub.execute_input":"2021-12-11T23:35:44.196307Z","iopub.status.idle":"2021-12-11T23:36:01.298236Z","shell.execute_reply.started":"2021-12-11T23:35:44.196266Z","shell.execute_reply":"2021-12-11T23:36:01.297473Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Create data loaders for test and validation sets","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:37:17.745863Z","iopub.execute_input":"2021-12-11T23:37:17.746691Z","iopub.status.idle":"2021-12-11T23:37:17.761593Z","shell.execute_reply.started":"2021-12-11T23:37:17.746639Z","shell.execute_reply":"2021-12-11T23:37:17.760905Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Define model initialization class ","metadata":{}},{"cell_type":"code","source":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False, version=\"mini\"):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in = 256 if version == \"mini\" else 768\n        H, D_out = 50, 2\n\n        # Instantiate BERT model\n        self.bert = AutoModel.from_pretrained(\"asafaya/bert-mini-arabic\") if version == \"mini\" else AutoModel.from_pretrained(\"asafaya/bert-base-arabic\")\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:38:37.634016Z","iopub.execute_input":"2021-12-11T23:38:37.634330Z","iopub.status.idle":"2021-12-11T23:38:37.661037Z","shell.execute_reply.started":"2021-12-11T23:38:37.634299Z","shell.execute_reply":"2021-12-11T23:38:37.660259Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nfrom torch.optim import SparseAdam, Adam\ndef initialize_model(epochs=4, version=\"mini\"):\n    \n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(params=list(bert_classifier.parameters()),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:39:35.549196Z","iopub.execute_input":"2021-12-11T23:39:35.549756Z","iopub.status.idle":"2021-12-11T23:39:35.570756Z","shell.execute_reply.started":"2021-12-11T23:39:35.549707Z","shell.execute_reply":"2021-12-11T23:39:35.569971Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### Define model train and evaluate functions","metadata":{}},{"cell_type":"code","source":"import random\nimport time\nimport torch\nimport torch.nn as nn\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n   \n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n   \n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:41:55.599607Z","iopub.execute_input":"2021-12-11T23:41:55.599885Z","iopub.status.idle":"2021-12-11T23:41:55.620290Z","shell.execute_reply.started":"2021-12-11T23:41:55.599855Z","shell.execute_reply":"2021-12-11T23:41:55.619481Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Initialize and train model\nset_seed(42) \nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:42:25.996197Z","iopub.execute_input":"2021-12-11T23:42:25.996903Z","iopub.status.idle":"2021-12-11T23:46:23.212481Z","shell.execute_reply.started":"2021-12-11T23:42:25.996867Z","shell.execute_reply":"2021-12-11T23:46:23.211740Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Saving the model for future runs\n\nimport pickle\nfilename = 'trained_model_mini_with_emojis.sav'\npickle.dump(bert_classifier, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:50:09.344010Z","iopub.execute_input":"2021-12-11T23:50:09.344747Z","iopub.status.idle":"2021-12-11T23:50:09.434681Z","shell.execute_reply.started":"2021-12-11T23:50:09.344706Z","shell.execute_reply":"2021-12-11T23:50:09.433965Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# # Loading the model (to avoid retraining in reruns)\n\n# import pickle\n# filename = 'trained_model_mini_with_emojis.sav'\n# f = open(filename, 'rb')\n# bert_classifier = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:51:09.236105Z","iopub.execute_input":"2021-12-11T23:51:09.236791Z","iopub.status.idle":"2021-12-11T23:51:09.240051Z","shell.execute_reply.started":"2021-12-11T23:51:09.236753Z","shell.execute_reply":"2021-12-11T23:51:09.239208Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Define prediction and test set evaluation functions\nimport torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n   \n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:51:38.473921Z","iopub.execute_input":"2021-12-11T23:51:38.474628Z","iopub.status.idle":"2021-12-11T23:51:38.481758Z","shell.execute_reply.started":"2021-12-11T23:51:38.474587Z","shell.execute_reply":"2021-12-11T23:51:38.481041Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true, model_name, dataset_name, test_dataset_name):\n    \n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title(f\" ROC of {model_name}  trained on {dataset_name} dataset & evaluated on the {test_dataset_name} dataset \")\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:52:06.218784Z","iopub.execute_input":"2021-12-11T23:52:06.219043Z","iopub.status.idle":"2021-12-11T23:52:06.226215Z","shell.execute_reply.started":"2021-12-11T23:52:06.219013Z","shell.execute_reply":"2021-12-11T23:52:06.225508Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Predict and evaluate validation subset\n# Compute predicted probabilities on the validation set\nprobs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, y_val, \"BERT-mini\", \"arabic-sentiment-twitter-corpus\", \"arabic-sentiment-twitter-corpus validation\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:52:30.407862Z","iopub.execute_input":"2021-12-11T23:52:30.408137Z","iopub.status.idle":"2021-12-11T23:52:33.753629Z","shell.execute_reply.started":"2021-12-11T23:52:30.408105Z","shell.execute_reply":"2021-12-11T23:52:33.752919Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Predict and evaluate test subset\n# Run `preprocessing_for_bert` on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(X_test)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:53:11.366582Z","iopub.execute_input":"2021-12-11T23:53:11.366842Z","iopub.status.idle":"2021-12-11T23:53:15.661470Z","shell.execute_reply.started":"2021-12-11T23:53:11.366813Z","shell.execute_reply":"2021-12-11T23:53:15.660750Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.5\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted non-negative\nprint(\"no-negative tweets ratio \", preds.sum()/len(preds))","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:53:22.745261Z","iopub.execute_input":"2021-12-11T23:53:22.745539Z","iopub.status.idle":"2021-12-11T23:53:30.152092Z","shell.execute_reply.started":"2021-12-11T23:53:22.745498Z","shell.execute_reply":"2021-12-11T23:53:30.151249Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Evaluate the Bert classifier for unseen test data\nevaluate_roc(probs, y_test,\"BERT-mini\", \"arabic-sentiment-twitter-corpus\",\"arabic-sentiment-twitter-corpus test\")","metadata":{"execution":{"iopub.status.busy":"2021-12-11T23:53:33.516054Z","iopub.execute_input":"2021-12-11T23:53:33.516685Z","iopub.status.idle":"2021-12-11T23:53:33.732739Z","shell.execute_reply.started":"2021-12-11T23:53:33.516648Z","shell.execute_reply":"2021-12-11T23:53:33.731945Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}